# A-Constitutional-AI-Faithfulness-Detector
This project investigates faithfulness in chain-of-thought (CoT) reasoning by exploring whether unfaithful reasoning can be isolated as a representational direction within large language models. We propose fine-tuning a base model on datasets with deliberately unfaithful reasoning to induce systematic deviations, then applying model diffing methods to identify features or directions correlated with unfaithfulness. These representations will be used both for steering (amplifying or suppressing faithfulness in CoT) and for detection, by developing algorithms that flag deceptive reasoning through hidden state activations, attention patterns, or causal interventions. Faithfulness will be measured and aggregated to capture model-level behavior. By characterizing the difference between faithful and unfaithful CoT, this work aims to provide tools for building models with more transparent reasoning and supports ongoing efforts, such as Anthropicâ€™s Constitutional AI, to ensure externalized reasoning is genuinely aligned rather than superficially compliant.
